{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "data = mnist.load_data()\n",
    "(X_train, y_train), (X_test, y_test) = data\n",
    "X_train = X_train.reshape((X_train.shape[0], 28 * 28)).astype(\"float32\")\n",
    "X_test = X_test.reshape((X_test.shape[0], 28 * 28)).astype(\"float32\")\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[16:13:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "{'subsample': 0.7999999999999999, 'num_class': 10, 'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.2, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.7999999999999999}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"objective\": [\"multi:softmax\"],\n",
    "    \"tree_method\":[\"hist\"],\n",
    "    \"max_depth\": [3, 6, 10, 15],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2, 0.3, 0.4],\n",
    "    \"subsample\": np.arange(0.5, 1.0, 0.1),\n",
    "    \"colsample_bytree\": np.arange(0.5, 1.0, 0.1),\n",
    "    \"colsample_bylevel\": np.arange(0.5, 1.0, 0.1),\n",
    "    \"n_estimators\": [100, 250, 500, 750],\n",
    "    \"num_class\": [10],    \n",
    "}\n",
    "\n",
    "xgbclf = XGBClassifier(random_state=0, verbosity=0)\n",
    "clf = RandomizedSearchCV(\n",
    "    estimator=xgbclf,\n",
    "    param_distributions=params,\n",
    "    scoring=\"accuracy\",\n",
    "    n_iter=25,\n",
    "    n_jobs=-1,\n",
    "    verbose=5,\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators': [400, 700, 1000],\n",
    "    'colsample_bytree': [0.7, 0.8],\n",
    "    'max_depth': [15,20,25],\n",
    "    'num_leaves': [50, 100, 200],\n",
    "    'reg_alpha': [1.1, 1.2, 1.3],\n",
    "    'reg_lambda': [1.1, 1.2, 1.3],\n",
    "    'min_split_gain': [0.3, 0.4],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'subsample_freq': [20]\n",
    "}\n",
    "\n",
    "lgbclf = LGBMClassifier(random_state=0)\n",
    "clf = RandomizedSearchCV(\n",
    "    estimator=lgbclf,\n",
    "    param_distributions=params,\n",
    "    scoring=\"accuracy\",\n",
    "    n_iter=25,\n",
    "    n_jobs=-1,\n",
    "    verbose=5,\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"C\": [0.1, 1, 10, 100, 1000],\n",
    "    \"gamma\": [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "    \"kernel\": [\"rbf\", \"poly\", \"sigmoid\"],\n",
    "}\n",
    "\n",
    "svmclf = SVC(random_state=0)\n",
    "clf = RandomizedSearchCV(\n",
    "    estimator=svmclf,\n",
    "    param_distributions=params,\n",
    "    scoring=\"accuracy\",\n",
    "    n_iter=25,\n",
    "    n_jobs=-1,\n",
    "    verbose=5,\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7679b3f13171ca1df491d4b5b53694221155d4305fc1b0a8de727533012e72db"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
